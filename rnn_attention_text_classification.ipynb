{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention for text classification on amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your path here\n",
    "PATH = Path(\"/data2/yinterian/Amazon_review_2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at `pre_processing_amazon_reviews.ipynb` for pre-processing steps for amazon review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(PATH/\"vocab123.pickle\", \"rb\"))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.reviews = df.review.values\n",
    "        self.summaries = df.summary.values\n",
    "        self.new_classes = {1:0, 2:0, 3:1, 4:2, 5:2}\n",
    "        self.targets = [self.new_classes[x] for x in df.rating.values]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        summary = self.summaries[idx]\n",
    "        review = self.reviews[idx]\n",
    "        words = summary + [word for sent in review for word in sent]\n",
    "        target = self.targets[idx]\n",
    "        return words, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open(PATH/\"Train123.pickle\", \"rb\"))\n",
    "valid_df = pickle.load(open(PATH/\"Valid123.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4799998, 3), (600000, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1237, 2102, 1463, 377, 286, 3089, 304, 2], [...</td>\n",
       "      <td>[842, 485]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[37311, 0, 25, 347, 3, 4987, 332, 2681, 303, ...</td>\n",
       "      <td>[332, 2681, 303]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[13, 318, 4], [1370, 6, 2], [207, 64, 10, 418...</td>\n",
       "      <td>[1370]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[336, 143, 8, 2], [14, 50, 1018, 3686, 5491, ...</td>\n",
       "      <td>[7, 1123, 3, 1633, 15383]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1920, 63, 49809, 2055, 1841, 3, 193, 2], [35...</td>\n",
       "      <td>[87]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  [[1237, 2102, 1463, 377, 286, 3089, 304, 2], [...   \n",
       "1  [[37311, 0, 25, 347, 3, 4987, 332, 2681, 303, ...   \n",
       "2  [[13, 318, 4], [1370, 6, 2], [207, 64, 10, 418...   \n",
       "3  [[336, 143, 8, 2], [14, 50, 1018, 3686, 5491, ...   \n",
       "4  [[1920, 63, 49809, 2055, 1841, 3, 193, 2], [35...   \n",
       "\n",
       "                     summary  rating  \n",
       "0                 [842, 485]       5  \n",
       "1           [332, 2681, 303]       2  \n",
       "2                     [1370]       5  \n",
       "3  [7, 1123, 3, 1633, 15383]       5  \n",
       "4                       [87]       3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(train_df[:600000])\n",
    "valid_ds = ReviewsDataset(valid_df[:60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([842,\n",
       "  485,\n",
       "  1237,\n",
       "  2102,\n",
       "  1463,\n",
       "  377,\n",
       "  286,\n",
       "  3089,\n",
       "  304,\n",
       "  2,\n",
       "  33,\n",
       "  61,\n",
       "  5,\n",
       "  251,\n",
       "  5,\n",
       "  193,\n",
       "  2],\n",
       " 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    seqs = [torch.LongTensor(sample[0]) for sample in batch]    \n",
    "    targets = torch.LongTensor([sample[1] for sample in batch])\n",
    "    seqlens = np.array([len(seq) for seq in seqs])\n",
    "    \n",
    "    # pad the batch and re-order \n",
    "    padded_seq = torch.zeros(seqlens.shape[0], int(seqlens.max())).long()\n",
    "    for idx, length in enumerate(seqlens):\n",
    "        padded_seq[idx, :length] = seqs[idx]\n",
    "    indices = np.argsort(-seqlens)\n",
    "    seqlens = seqlens[indices]\n",
    "    padded_seq = padded_seq[torch.LongTensor(indices)]\n",
    "    targets = targets[torch.LongTensor(indices)]\n",
    "    \n",
    "    return (padded_seq, seqlens, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_ds[0], train_ds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  332,  2681,   303, 37311,     0,    25,   347,     3,  4987,   332,\n",
       "           2681,   303,     2,  2681,  4114,  1437, 13787,  2253,  6143,     2,\n",
       "             26,  4032,   347,  2689,     2,  1073,  3319,   602, 60148,  1718,\n",
       "            447,     3,   255,   347,  1224,   602,     2],\n",
       "         [  842,   485,  1237,  2102,  1463,   377,   286,  3089,   304,     2,\n",
       "             33,    61,     5,   251,     5,   193,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]),\n",
       " array([37, 17]),\n",
       " tensor([0, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=3, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, seqlens, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1370,    13,   318,     4,  1370,     6,     2,   207,    64,    10,\n",
       "           418,     2,   119, 12511,    43,  5121,  1166,     2,   400,  6887,\n",
       "             9, 10027,  1289,   977,    43,  1196,     4,   174,    87,     2,\n",
       "           188,  1314,    87,    91,    87,   127,   675,    95,     2],\n",
       "        [  332,  2681,   303, 37311,     0,    25,   347,     3,  4987,   332,\n",
       "          2681,   303,     2,  2681,  4114,  1437, 13787,  2253,  6143,     2,\n",
       "            26,  4032,   347,  2689,     2,  1073,  3319,   602, 60148,  1718,\n",
       "           447,     3,   255,   347,  1224,   602,     2,     0,     0],\n",
       "        [  842,   485,  1237,  2102,  1463,   377,   286,  3089,   304,     2,\n",
       "            33,    61,     5,   251,     5,   193,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(vocab_size, 10, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(10, 5, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 39, 10])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_emb = emb(seq)\n",
    "seq_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pack = pack_padded_sequence(seq_emb, seqlens, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pack, ht = gru(seq_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pad, lens = pad_packed_sequence(out_pack, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 39, 10])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([ht[0], ht[1]], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attention module\n",
    "mlp = nn.Sequential(nn.Linear(5, 5), nn.Tanh())\n",
    "context_vector = nn.Parameter(torch.Tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = mlp(out_pad)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = F.softmax(u.matmul(context_vector), dim=1)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 37])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = attn_weight.unsqueeze(1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.bmm(a, u)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out[:,0,:]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $w_t$ be the index of word $t$ in a review.\n",
    "\n",
    "**Step 1:**\n",
    "Apply embedding to $w_t$,\n",
    "$x_t$ = Embddding($w_t$)\n",
    "\n",
    "**Step 2:** Take the sequence $x = (x_1, \\dots, x_T)$ through a GRU layer. $h_1, \\dots h_T = GRU(x_1, \\dots, x_T)$.\n",
    "$h_t$ is a representation of the information around $w_t$.\n",
    "\n",
    "**Step 3: Attention** Not all words contribute equally to the representation of the sentece meaning. We compute attention on each world. \n",
    "\n",
    "$u_t = tanh(Wh_t + b)$\n",
    "\n",
    "$\\alpha_t = \\frac{exp(u_t u_w)}{\\sum_j exp(u_j u_w) }$\n",
    "\n",
    "$s = \\sum_t \\alpha_t h_t$\n",
    "\n",
    "**Step 4:** Linear layer on $s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Tanh())\n",
    "        self.context_vector = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = self.mlp(x)\n",
    "        alpha = F.softmax(u.matmul(self.context_vector), dim=1)\n",
    "        weighted_out = torch.bmm(alpha.unsqueeze(1), u)\n",
    "        return  weighted_out[:,0,:], alpha\n",
    "\n",
    "\n",
    "class RNNattention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=50, output_dim=3, dropout=0.2, bi=False):\n",
    "        super(RNNattention, self).__init__()\n",
    "        self.dropout_p = dropout\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=bi)\n",
    "        self.attention = AttentionModule(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, seqs, seqlens):\n",
    "        embs = self.emb(seqs)\n",
    "        if self.dropout_p > 0.0:\n",
    "            embs = self.dropout(embs)\n",
    "        seq_pack = pack_padded_sequence(embs, seqlens, batch_first=True)\n",
    "        out_pack, _ = self.gru(seq_pack)\n",
    "        out_pad, _ = pad_packed_sequence(out_pack, batch_first=True)\n",
    "        att_out, alpha = self.attention(out_pad)  \n",
    "        outputs = self.classifier(att_out)    \n",
    "        return outputs, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBI(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=50, output_dim=3, bi=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=bi)\n",
    "        self.classifier = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, seqs, seqlens):\n",
    "        embs = self.emb(seqs)\n",
    "        embs = self.dropout(embs)\n",
    "        seq_pack = pack_padded_sequence(embs, seqlens, batch_first=True)\n",
    "        _, ht = self.gru(seq_pack)\n",
    "        ht = torch.cat([ht[0], ht[1]], 1)\n",
    "        outputs = self.classifier(ht)    \n",
    "        return outputs, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=50, output_dim=3, bi=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=bi)\n",
    "        self.classifier = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, seqs, seqlens):\n",
    "        embs = self.emb(seqs)\n",
    "        embs = self.dropout(embs)\n",
    "        seq_pack = pack_padded_sequence(embs, seqlens, batch_first=True)\n",
    "        _, ht = self.gru(seq_pack)\n",
    "        ht = torch.cat([ht[0], ht[1]], 1)\n",
    "        outputs = self.classifier(ht)    \n",
    "        return outputs, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size, bi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, seqlens, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = model(seqs, seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0965, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, train_dl, valid_dl, optimizer, epochs=10):\n",
    "    iterations = epochs*len(train_dl)\n",
    "    pbar = tqdm(total=iterations)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_pred, _ = model(x, s)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            pbar.update()\n",
    "        val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_hat, _ = model(x, s)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        preds = torch.max(y_hat, dim=1)[1]\n",
    "        correct += (preds==y).float().sum().item()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing, start with less data\n",
    "train_ds = ReviewsDataset(train_df)\n",
    "valid_ds = ReviewsDataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf840400d6d9482ca4c179eea3a66b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.339 val loss 0.305 and val accuracy 0.887\n",
      "train loss 0.308 val loss 0.294 and val accuracy 0.890\n",
      "train loss 0.296 val loss 0.284 and val accuracy 0.894\n",
      "train loss 0.288 val loss 0.278 and val accuracy 0.897\n",
      "train loss 0.281 val loss 0.275 and val accuracy 0.897\n",
      "train loss 0.276 val loss 0.271 and val accuracy 0.899\n",
      "train loss 0.271 val loss 0.266 and val accuracy 0.902\n",
      "train loss 0.267 val loss 0.263 and val accuracy 0.902\n",
      "train loss 0.264 val loss 0.260 and val accuracy 0.904\n",
      "train loss 0.261 val loss 0.258 and val accuracy 0.905\n"
     ]
    }
   ],
   "source": [
    "# training baseline model\n",
    "model = RNN(vocab_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62161878d00433d9a559af8406ec887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training bidirectional baseline model\n",
    "model = RNN(vocab_size, bi=True).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with learning rates and dropout\n",
    "Bottom line: the right hyperparameters are supper important. I used a small subset of the dats here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(train_df[:n])\n",
    "valid_ds = ReviewsDataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNattention(vocab_size, dropout=0.0).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b8896d6c5b47c0b5a6ca772e058ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n",
      "train loss nan val loss nan and val accuracy 0.132\n"
     ]
    }
   ],
   "source": [
    "# without dropout and with a small learning rate\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb0bc5768174d28a1f87e8c30446c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.419 val loss 0.337 and val accuracy 0.875\n",
      "train loss 0.316 val loss 0.319 and val accuracy 0.882\n",
      "train loss 0.273 val loss 0.317 and val accuracy 0.886\n",
      "train loss 0.231 val loss 0.328 and val accuracy 0.886\n",
      "train loss 0.193 val loss 0.338 and val accuracy 0.887\n",
      "train loss 0.162 val loss 0.371 and val accuracy 0.880\n",
      "train loss 0.138 val loss 0.387 and val accuracy 0.885\n",
      "train loss 0.121 val loss 0.438 and val accuracy 0.883\n",
      "train loss 0.108 val loss 0.448 and val accuracy 0.885\n",
      "train loss 0.097 val loss 0.470 and val accuracy 0.886\n"
     ]
    }
   ],
   "source": [
    "model = RNNattention(vocab_size, dropout=0.0).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1e1a83ce2a44a19ed2f0cfd19db14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.461 val loss 0.356 and val accuracy 0.870\n",
      "train loss 0.346 val loss 0.334 and val accuracy 0.878\n",
      "train loss 0.315 val loss 0.324 and val accuracy 0.881\n",
      "train loss 0.291 val loss 0.320 and val accuracy 0.883\n",
      "train loss 0.271 val loss 0.314 and val accuracy 0.886\n",
      "train loss 0.251 val loss 0.320 and val accuracy 0.888\n",
      "train loss 0.233 val loss 0.332 and val accuracy 0.888\n",
      "train loss 0.217 val loss 0.332 and val accuracy 0.889\n",
      "train loss 0.205 val loss 0.343 and val accuracy 0.889\n",
      "train loss 0.192 val loss 0.362 and val accuracy 0.887\n"
     ]
    }
   ],
   "source": [
    "# with dropout 0.2\n",
    "model = RNNattention(vocab_size, dropout=0.2).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c2fcaed694e65ad035c9e3d76c35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.457 val loss 0.347 and val accuracy 0.873\n",
      "train loss 0.341 val loss 0.329 and val accuracy 0.878\n",
      "train loss 0.317 val loss 0.323 and val accuracy 0.881\n",
      "train loss 0.298 val loss 0.319 and val accuracy 0.884\n",
      "train loss 0.282 val loss 0.315 and val accuracy 0.886\n",
      "train loss 0.266 val loss 0.319 and val accuracy 0.887\n",
      "train loss 0.251 val loss 0.320 and val accuracy 0.888\n",
      "train loss 0.237 val loss 0.323 and val accuracy 0.888\n",
      "train loss 0.225 val loss 0.323 and val accuracy 0.889\n",
      "train loss 0.213 val loss 0.335 and val accuracy 0.889\n"
     ]
    }
   ],
   "source": [
    "# with dropout 0.2\n",
    "model = RNNattention(vocab_size, dropout=0.2).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(train_df)\n",
    "valid_ds = ReviewsDataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101299be23ca452387011dd8247f4a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.343 val loss 0.308 and val accuracy 0.887\n",
      "train loss 0.309 val loss 0.295 and val accuracy 0.890\n",
      "train loss 0.298 val loss 0.288 and val accuracy 0.893\n",
      "train loss 0.292 val loss 0.287 and val accuracy 0.894\n",
      "train loss 0.286 val loss 0.280 and val accuracy 0.896\n",
      "train loss 0.281 val loss 0.277 and val accuracy 0.897\n",
      "train loss 0.277 val loss 0.276 and val accuracy 0.897\n",
      "train loss 0.274 val loss 0.272 and val accuracy 0.899\n",
      "train loss 0.271 val loss 0.270 and val accuracy 0.900\n",
      "train loss 0.268 val loss 0.272 and val accuracy 0.899\n"
     ]
    }
   ],
   "source": [
    "model = RNNattention(vocab_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27222656562924386 0.898875 /data2/yinterian/Amazon_review_2014/models/model_att_acc_90.pth\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "path = \"{0}/models/model_att_acc_{1:.0f}.pth\".format(PATH, 100*val_acc) \n",
    "save_model(model, path)\n",
    "print(val_loss, val_acc, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff881b3a08914ff4992f4ce0ba17c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.261 val loss 0.262 and val accuracy 0.903\n",
      "train loss 0.258 val loss 0.260 and val accuracy 0.904\n",
      "train loss 0.255 val loss 0.259 and val accuracy 0.905\n",
      "train loss 0.253 val loss 0.258 and val accuracy 0.905\n",
      "train loss 0.251 val loss 0.255 and val accuracy 0.906\n",
      "train loss 0.249 val loss 0.254 and val accuracy 0.907\n",
      "train loss 0.247 val loss 0.252 and val accuracy 0.907\n",
      "train loss 0.245 val loss 0.252 and val accuracy 0.908\n",
      "train loss 0.243 val loss 0.254 and val accuracy 0.908\n",
      "train loss 0.241 val loss 0.250 and val accuracy 0.909\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24984130665659904 0.90873 /data2/yinterian/Amazon_review_2014/models/model_att_acc_91.pth\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "path = \"{0}/models/model_att_acc_{1:.0f}.pth\".format(PATH, 100*val_acc) \n",
    "save_model(model, path)\n",
    "print(val_loss, val_acc, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def colorize(words, color_array):\n",
    "    # words is a list of words\n",
    "    # color_array is an array of numbers between 0 and 1 of length equal to words\n",
    "    cmap=matplotlib.cm.Blues\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
    "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    return colored_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNattention(\n",
       "  (emb): Embedding(282174, 100, padding_idx=0)\n",
       "  (gru): GRU(100, 50, batch_first=True)\n",
       "  (attention): AttentionModule(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNattention(vocab_size).cuda()\n",
    "path = '/data2/yinterian/Amazon_review_2014/models/model_att_acc_91.pth'\n",
    "load_model(model, path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,s,y = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0937e-05, 5.7472e-04, 9.9937e-01]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, alpha0 = model(x.cuda(), s)\n",
    "F.softmax(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   7,   32,    7,   17,   40,  856, 8930, 3983,  548,    2, 1791, 2986,\n",
       "             7, 2986,  149,    2]]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['great', 'price', 'great', 'product', 'perfect', 'keeping', 'pup',\n",
       "       'cozy', 'dry', '.', 'snap', 'hood', 'great', 'hood', 'needed', '.'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array([vocab.itos[w] for w in x[0].numpy()])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = alpha[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = colorize(words, len(words)/2.5*attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"barcode\"; style=\"color: black; background-color: #08306b\">&nbspgreat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #74b3d8\">&nbspprice&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #4292c6\">&nbspgreat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f0f6fd\">&nbspproduct&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #84bcdb\">&nbspperfect&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #92c4de\">&nbspkeeping&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #3888c1\">&nbsppup&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f2f7fd\">&nbspcozy&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #d6e6f4\">&nbspdry&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #6fb0d7\">&nbsp.&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #d3e3f3\">&nbspsnap&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #69add5\">&nbsphood&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #84bcdb\">&nbspgreat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #e4eff9\">&nbsphood&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #5fa6d1\">&nbspneeded&nbsp</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20435348, 0.07452422, 0.09727789, 0.00580973, 0.06881874,\n",
       "       0.06306457, 0.10320041, 0.00465984, 0.0258216 , 0.07656651,\n",
       "       0.02928499, 0.07915612, 0.06866734, 0.0147968 , 0.08399773],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Hierarchical Attention Networks for Document Classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy\n",
    "* https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/attention.html\n",
    "* https://gist.github.com/ihsgnef/f13c35cd46624c8f458a4d23589ac768\n",
    "* https://stackoverflow.com/questions/59220488/to-visualize-attention-color-tokens-using-attention-weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
