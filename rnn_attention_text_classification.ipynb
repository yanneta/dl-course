{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention for text classification on amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your path here\n",
    "PATH = Path(\"/data2/yinterian/Amazon_review_2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at `pre_processing_amazon_reviews.ipynb` for pre-processing steps for amazon review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(PATH/\"vocab123.pickle\", \"rb\"))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.reviews = df.review.values\n",
    "        self.summaries = df.summary.values\n",
    "        self.new_classes = {1:0, 2:0, 3:1, 4:2, 5:2}\n",
    "        self.targets = [self.new_classes[x] for x in df.rating.values]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        summary = self.summaries[idx]\n",
    "        review = self.reviews[idx]\n",
    "        words = summary + [word for sent in review for word in sent]\n",
    "        target = self.targets[idx]\n",
    "        return words, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open(PATH/\"Train123.pickle\", \"rb\"))\n",
    "valid_df = pickle.load(open(PATH/\"Valid123.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1237, 2102, 1463, 377, 286, 3089, 304, 2], [...</td>\n",
       "      <td>[842, 485]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[37311, 0, 25, 347, 3, 4987, 332, 2681, 303, ...</td>\n",
       "      <td>[332, 2681, 303]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[13, 318, 4], [1370, 6, 2], [207, 64, 10, 418...</td>\n",
       "      <td>[1370]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[336, 143, 8, 2], [14, 50, 1018, 3686, 5491, ...</td>\n",
       "      <td>[7, 1123, 3, 1633, 15383]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1920, 63, 49809, 2055, 1841, 3, 193, 2], [35...</td>\n",
       "      <td>[87]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  [[1237, 2102, 1463, 377, 286, 3089, 304, 2], [...   \n",
       "1  [[37311, 0, 25, 347, 3, 4987, 332, 2681, 303, ...   \n",
       "2  [[13, 318, 4], [1370, 6, 2], [207, 64, 10, 418...   \n",
       "3  [[336, 143, 8, 2], [14, 50, 1018, 3686, 5491, ...   \n",
       "4  [[1920, 63, 49809, 2055, 1841, 3, 193, 2], [35...   \n",
       "\n",
       "                     summary  rating  \n",
       "0                 [842, 485]       5  \n",
       "1           [332, 2681, 303]       2  \n",
       "2                     [1370]       5  \n",
       "3  [7, 1123, 3, 1633, 15383]       5  \n",
       "4                       [87]       3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(train_df)\n",
    "valid_ds = ReviewsDataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([842,\n",
       "  485,\n",
       "  1237,\n",
       "  2102,\n",
       "  1463,\n",
       "  377,\n",
       "  286,\n",
       "  3089,\n",
       "  304,\n",
       "  2,\n",
       "  33,\n",
       "  61,\n",
       "  5,\n",
       "  251,\n",
       "  5,\n",
       "  193,\n",
       "  2],\n",
       " 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    seqs = [torch.LongTensor(sample[0]) for sample in batch]    \n",
    "    targets = torch.LongTensor([sample[1] for sample in batch])\n",
    "    seqlens = np.array([len(seq) for seq in seqs])\n",
    "    \n",
    "    # pad the batch and re-order \n",
    "    padded_seq = torch.zeros(seqlens.shape[0], int(seqlens.max())).long()\n",
    "    for idx, length in enumerate(seqlens):\n",
    "        padded_seq[idx, :length] = seqs[idx]\n",
    "    indices = np.argsort(-seqlens)\n",
    "    seqlens = seqlens[indices]\n",
    "    padded_seq = padded_seq[torch.LongTensor(indices)]\n",
    "    targets = targets[torch.LongTensor(indices)]\n",
    "    \n",
    "    return (padded_seq, seqlens, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_ds[0], train_ds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  332,  2681,   303, 37311,     0,    25,   347,     3,  4987,   332,\n",
       "           2681,   303,     2,  2681,  4114,  1437, 13787,  2253,  6143,     2,\n",
       "             26,  4032,   347,  2689,     2,  1073,  3319,   602, 60148,  1718,\n",
       "            447,     3,   255,   347,  1224,   602,     2],\n",
       "         [  842,   485,  1237,  2102,  1463,   377,   286,  3089,   304,     2,\n",
       "             33,    61,     5,   251,     5,   193,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]),\n",
       " array([37, 17]),\n",
       " tensor([0, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, seqlens, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  332,  2681,   303, 37311,     0,    25,   347,     3,  4987,   332,\n",
       "          2681,   303,     2,  2681,  4114,  1437, 13787,  2253,  6143,     2,\n",
       "            26,  4032,   347,  2689,     2,  1073,  3319,   602, 60148,  1718,\n",
       "           447,     3,   255,   347,  1224,   602,     2],\n",
       "        [  842,   485,  1237,  2102,  1463,   377,   286,  3089,   304,     2,\n",
       "            33,    61,     5,   251,     5,   193,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(vocab_size, 10, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(10, 5, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_emb = emb(seq)\n",
    "seq_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pack = pack_padded_sequence(seq_emb, seqlens, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pack, _ = gru(seq_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pad, lens = pad_packed_sequence(out_pack, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attention module\n",
    "mlp = nn.Sequential(nn.Linear(5, 5), nn.Tanh())\n",
    "context_vector = nn.Parameter(torch.Tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = mlp(out_pad)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = F.softmax(u.matmul(context_vector), dim=1)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 37])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = attn_weight.unsqueeze(1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.bmm(a, u)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out[:,0,:]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $w_t$ be the index of word $t$ in a review.\n",
    "\n",
    "**Step 1:**\n",
    "Apply embedding to $w_t$,\n",
    "$x_t$ = Embddding($w_t$)\n",
    "\n",
    "**Step 2:** Take the sequence $x = (x_1, \\dots, x_T)$ through a GRU layer. $h_1, \\dots h_T = GRU(x_1, \\dots, x_T)$.\n",
    "$h_t$ is a representation of the information around $w_t$.\n",
    "\n",
    "**Step 3: Attention** Not all words contribute equally to the representation of the sentece meaning. We compute attention on each world. \n",
    "\n",
    "$u_t = tanh(Wh_t + b)$\n",
    "\n",
    "$\\alpha_t = \\frac{exp(u_t u_w)}{\\sum_j exp(u_j u_w) }$\n",
    "\n",
    "$s = \\sum_t \\alpha_t h_t$\n",
    "\n",
    "**Step 4:** Linear layer on $s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Tanh())\n",
    "        self.context_vector = nn.Parameter(torch.Tensor(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = self.mlp(x)\n",
    "        alpha = F.softmax(u.matmul(self.context_vector), dim=1)\n",
    "        weighted_out = torch.bmm(alpha.unsqueeze(1), u)\n",
    "        return  weighted_out[:,0,:]\n",
    "\n",
    "\n",
    "class RNNattention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=50, output_dim=3):\n",
    "        super(RNNattention, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = AttentionModule(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, seqs, seqlens):\n",
    "        \n",
    "        embs = self.emb(seqs)\n",
    "        seq_pack = pack_padded_sequence(embs, seqlens, batch_first=True)\n",
    "        out_pack, _ = self.gru(seq_pack)\n",
    "        out_pad, _ = pad_packed_sequence(out_pack, batch_first=True)\n",
    "        att_out = self.attention(out_pad)  \n",
    "        outputs = self.classifier(att_out)    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNattention(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, seqlens, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(seqs, seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0935, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, train_dl, valid_dl, optimizer, epochs=10):\n",
    "    iterations = epochs*len(train_dl)\n",
    "    pbar = tqdm(total=iterations)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_pred = model(x, s)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            pbar.update()\n",
    "        val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_hat = model(x, s)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        preds = torch.max(y_hat, dim=1)[1]\n",
    "        correct += (preds==targs).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10000, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10000, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNattention(vocab_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n",
    "                                  keep_batchnorm_fp32=True, loss_scale=\"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb9250e362e452aaa6abc96a21c95fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_epocs(model, train_dl, valid_dl, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Hierarchical Attention Networks for Document Classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy\n",
    "* https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/attention.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
