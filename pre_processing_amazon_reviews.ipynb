{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import math\n",
    "import json\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your path here\n",
    "PATH = Path(\"/data2/yinterian/Amazon_review_2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0**: Get the data. The zip file is 18G it make take 20 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget http://snap.stanford.edu/data/amazon/productGraph/aggressive_dedup.json.gz -P $PATH\n",
    "    ! gunzip $PATH/aggressive_dedup.json.gz \n",
    "#unpack_dataset()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 yinterian yinterian 55G Feb 17  2016 /data2/yinterian/Amazon_review_2014/data/aggressive_dedup.json\r\n"
     ]
    }
   ],
   "source": [
    "# the file is 55G\n",
    "!ls -lhS  $PATH/data/aggressive_dedup.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Split file into smaller chunks:\n",
    "\n",
    "I splitted into 42 files of around 1.3G each. This is very slow and you should run it on your terminal. You need to install `jq`\n",
    "\n",
    "`jq -c < $PATH/aggressive_dedup.json | split -l 2000000 `\n",
    "\n",
    "Here are the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/yinterian/Amazon_review_2014/data/xaa\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xab\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xac\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xad\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xae\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xaf\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xag\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xah\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xai\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xaj\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xak\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xal\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xam\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xan\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xao\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xap\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xaq\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xar\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xas\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xat\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xau\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xav\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xaw\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xax\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xay\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xaz\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xba\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbb\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbc\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbd\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbe\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbf\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbg\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbh\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbi\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbj\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbk\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbl\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbm\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbn\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbo\r\n",
      "/data2/yinterian/Amazon_review_2014/data/xbp\r\n"
     ]
    }
   ],
   "source": [
    "!ls  $PATH/data/x*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spep 2**: In order to preprocess the data I saved the following code in a file called pre-process.py (find it in the repo) and I ran each part of the original fine independently. Here is an example of how you run the first part. This is also slow because we are running spacy. \n",
    "\n",
    "`python pre_process_amz_reviews.py --input_file data/xaa --output_file amazon_reviews-01.json > out1 &`\n",
    "\n",
    "If you have many CPUs you can run a few in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "def load_data(datafile):\n",
    "    samples = [json.loads(line) for line in open(datafile).readlines()]\n",
    "    data = {}\n",
    "    data['review'] = [html.unescape(sample['reviewText']) for sample in samples]\n",
    "    data['summary'] = [html.unescape(sample['summary']) for sample in samples]\n",
    "    data['rating'] = np.array([sample['overall'] for sample in samples])\n",
    "    return data\n",
    "\n",
    "def get_clean_review(review, summ, rating):\n",
    "    sample = {}\n",
    "    # remove stop-words and whitespace tokens split paragraphs into sentences\n",
    "    review_valid = [[tok for tok in sent if not tok.is_stop and tok.text.strip() != ''] for sent in review.sents]\n",
    "    # remove empty sentences\n",
    "    review_valid = [sent for sent in review_valid if not len(sent) == 0]\n",
    "    sample['review'] = [[tok.text.lower() for tok in sent] for sent in review_valid]\n",
    "    # remove stop-words and whitespace tokens\n",
    "    summary_valid = [tok for tok in summ if not tok.is_stop and tok.text.strip() != '']\n",
    "    sample['summary'] = [tok.text.lower() for tok in summary_valid]\n",
    "    sample['rating'] = int(rating)\n",
    "    return sample\n",
    "\n",
    "def dump_dataset(raw_data, outfile, summary=True):\n",
    "    with open(outfile, 'w') as outf:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        review_docs = nlp.pipe(raw_data['review'])\n",
    "        summ_docs = nlp.pipe(raw_data['summary'])\n",
    "        n = len(raw_data['rating'])\n",
    "        pbar = tqdm(total=n)\n",
    "        for review, summ, rating in zip(review_docs, summ_docs, raw_data['rating']):\n",
    "            sample = get_clean_review(review, summ, rating)\n",
    "            pbar.update()\n",
    "            outf.write(json.dumps(sample) + '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Split in train/ valid/ test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(datafile):\n",
    "    samples = [json.loads(line) for line in open(datafile).readlines()]\n",
    "    return samples\n",
    "\n",
    "# here is a sample of 3  files\n",
    "#L1 = load_data_from_json(PATH/\"amazon_reviews-01.json\")\n",
    "#L2 = load_data_from_json(PATH/\"amazon_reviews-02.json\")\n",
    "#L3 = load_data_from_json(PATH/\"amazon_reviews-03.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(Ls):\n",
    "    train = np.array([])\n",
    "    valid = np.array([])\n",
    "    test = np.array([])\n",
    "    np.random.seed(seed=3)\n",
    "    for L in Ls:\n",
    "        L = np.array(L)\n",
    "        np.random.shuffle(L)\n",
    "        n1 = int(0.8*len(L))\n",
    "        n2 = int(0.9*len(L))\n",
    "        t1, t2, t3 = L[:n1], L1[n1:n2], L1[n2:]\n",
    "        train = np.concatenate((train, t1))\n",
    "        valid = np.concatenate((valid, t2))\n",
    "        test = np.concatenate((test, t3))\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a subset of the data\n",
    "#train, valid, test = split_data([L1, L2, L3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800000,), (600000,), (600000,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': [['inexpensive',\n",
       "   'method',\n",
       "   'club',\n",
       "   'face',\n",
       "   'clean',\n",
       "   'golf',\n",
       "   'course',\n",
       "   '.'],\n",
       "  ['buy', '2', '\"', 'wear', '\"', 'easily', '.']],\n",
       " 'summary': ['handy', 'tool'],\n",
       " 'rating': 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your files\n",
    "#pickle.dump(train, open(PATH/\"train123.pickle\", 'wb'))\n",
    "#pickle.dump(valid, open(PATH/\"valid123.pickle\", 'wb'))\n",
    "#pickle.dump(test, open(PATH/\"test123.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(PATH/\"train123.pickle\", \"rb\"))\n",
    "valid = pickle.load(open(PATH/\"valid123.pickle\", \"rb\"))\n",
    "test = pickle.load(open(PATH/\"test123.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800000,), (600000,), (600000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                              \n",
      "100%|█████████▉| 398721/400000 [00:28<00:00, 13645.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping vocab to /data2/yinterian/Amazon_review_2014/vocab123.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 398721/400000 [00:42<00:00, 13645.56it/s]"
     ]
    }
   ],
   "source": [
    "def build_comb_vocab(train, vocab_file):\n",
    "    reviews = [[word for sent in sample['review'] for word in sent] + sample['summary'] * 2 for sample in train]\n",
    "    review_field = Field()\n",
    "    review_field.build_vocab(reviews, min_freq=5, vectors=\"glove.6B.200d\")\n",
    "    print(\"Dumping vocab to {}\".format(vocab_file))\n",
    "    pickle.dump(review_field.vocab, open(vocab_file, 'wb'))\n",
    "    \n",
    "#build_comb_vocab(train, PATH/\"vocab123.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(PATH/\"vocab123.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282174"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2index(data, vocab, filename):\n",
    "    reviews = [sample['review'] for sample in data]\n",
    "    summaries = [sample['summary'] for sample in data]\n",
    "    ratings = [sample['rating'] for sample in data]\n",
    "    Reviews = [[[vocab.stoi[w] for w in sent] for sent in review if len(sent) > 0] for review in reviews]\n",
    "    Summaries = [[vocab.stoi[w] for w in summary] for summary in summaries]\n",
    "    df = pd.DataFrame({\"review\": Reviews, \"summary\": Summaries, \"rating\": ratings})\n",
    "    pickle.dump(df, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2index(train, vocab, PATH/\"Train123.pickle\")\n",
    "words2index(valid, vocab, PATH/\"Valid123.pickle\")\n",
    "words2index(test, vocab, PATH/\"Test123.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://github.com/Shivanshu-Gupta/hierarchical-attention-network <br>\n",
    "* Hierarchical Attention Networks for Document Classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
